from typing import List, Optional
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Document, Settings
from llama_index.core.node_parser import UnstructuredElementNodeParser
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.postprocessor import LongContextReorder
import qdrant_client
from .config_manager import ConfigManager


class LlamaIndexProcessor:
    """LlamaIndex æ–‡ä»¶è™•ç†å’ŒæŸ¥è©¢é¡ž"""
    
    def __init__(self, config_manager: Optional[ConfigManager] = None):
        self.config_manager = config_manager or ConfigManager()
        self.gemini_config = self.config_manager.get_gemini_config()
        self.qdrant_config = self.config_manager.get_qdrant_config()
        
        self._setup_models()
        
        self.index = None
        self.query_engine = None
    
    def _setup_models(self):
        """è¨­å®š LLM å’ŒåµŒå…¥æ¨¡åž‹"""
        # LLM è¨­å®š
        self.llm = Gemini(
            model_name=self.gemini_config['model_name'], 
            api_key=self.gemini_config['api_key']
        )
        
        # åµŒå…¥æ¨¡åž‹è¨­å®š
        self.embed_model = GoogleGenAIEmbedding(
            api_key=self.gemini_config['api_key'],
            model=self.gemini_config['embedding_model'],
            task_type="RETRIEVAL_DOCUMENT"
        )
        
        # å…¨åŸŸè¨­å®š
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        Settings.node_parser = UnstructuredElementNodeParser(llm=self.llm)
    
    def load_documents(self, input_dir: str, required_exts: List[str] = [".pdf"]) -> List[Document]:
        loader = SimpleDirectoryReader(
            input_dir=input_dir,
            recursive=True,
            required_exts=required_exts,
            exclude=["*.tmp"],
            encoding='utf-8'
        )
        return loader.load_data()
    
    def create_qdrant_index(self, documents: List[Document], collection_name: str = "document_collection") -> VectorStoreIndex:
        # Qdrant å®¢æˆ¶ç«¯é€£æŽ¥
        qdrant_client_instance = qdrant_client.QdrantClient(
            url=self.qdrant_config['url'],
            api_key=self.qdrant_config['api_key']
        )
        
        # æª¢æŸ¥ä¸¦åˆªé™¤ç¾æœ‰é›†åˆ
        try:
            qdrant_client_instance.get_collection(collection_name=collection_name)
            print(f"é›†åˆ '{collection_name}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆªé™¤...")
            qdrant_client_instance.delete_collection(collection_name=collection_name)
            print("åˆªé™¤æˆåŠŸã€‚")
        except Exception as e:
            print(f"é›†åˆ '{collection_name}' ä¸å­˜åœ¨ï¼Œç„¡éœ€åˆªé™¤ã€‚")
        
        # å»ºç«‹å‘é‡å­˜å„²
        vector_store = QdrantVectorStore(
            client=qdrant_client_instance,
            collection_name=collection_name,
            enable_hybrid=True
        )
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        
        # å»ºç«‹å‘é‡ç´¢å¼•
        self.index = VectorStoreIndex.from_documents(
            documents,
            storage_context=storage_context,
            vector_store_kwargs={"enable_hybrid": True},
            show_progress=True
        )
        
        return self.index
    
    def create_query_engine(self, 
                          vector_store_query_mode: str = 'hybrid',
                          alpha: float = 0.5,
                          similarity_top_k: int = 5,
                          sparse_top_k: int = 5,
                          num_queries: int = 4,
                          streaming: bool = True):
        if not self.index:
            raise ValueError("è«‹å…ˆå»ºç«‹ç´¢å¼•")
        
        chat_llm = Gemini(
            model_name=self.gemini_config['model_name'],
            api_key=self.gemini_config['api_key']
        )
        
        self.query_engine = self.index.as_query_engine(
            llm=chat_llm,
            vector_store_query_mode=vector_store_query_mode,
            alpha=alpha,
            similarity_top_k=similarity_top_k,
            sparse_top_k=sparse_top_k,
            node_postprocessors=[LongContextReorder()],
            num_queries=num_queries,
            streaming=streaming
        )
        
        return self.query_engine
    
    def query(self, question: str) -> str:
        if not self.query_engine:
            raise ValueError("è«‹å…ˆå»ºç«‹æŸ¥è©¢å¼•æ“Ž")
        
        response = self.query_engine.query(question)
        
        print(f"ðŸ” LlamaIndex æŸ¥è©¢: {question}")
        print(f"ðŸ“ éŸ¿æ‡‰é¡žåž‹: {type(response)}")
        
        # æª¢æŸ¥æ˜¯å¦æ˜¯ StreamingResponse
        if hasattr(response, 'response_gen'):
            print(f"âœ… æª¢æ¸¬åˆ° StreamingResponseï¼Œè¿”å›žå®Œæ•´å°è±¡")
            return response  # ç›´æŽ¥è¿”å›ž StreamingResponse å°è±¡
        else:
            # ä¸€èˆ¬æ¨¡å¼ - æª¢æŸ¥éŸ¿æ‡‰æ˜¯å¦æœ‰æ•ˆ
            print(f"ðŸ“ LlamaIndex åŽŸå§‹éŸ¿æ‡‰: {response}")
            if response is not None and str(response).strip():
                response_str = str(response)
                print(f"âœ… è¿”å›žéŸ¿æ‡‰: {response_str}")
                return response_str
            else:
                print(f"âš ï¸ éŸ¿æ‡‰ç‚ºç©ºæˆ–ç„¡æ•ˆ")
                return None  # è¿”å›ž None è€Œä¸æ˜¯å­—ä¸² "None"
    
    def process_documents_and_query(self, 
                                  input_dir: str, 
                                  question: str,
                                  collection_name: str = "document_collection",
                                  required_exts: List[str] = [".pdf"]) -> str:
        # è¼‰å…¥æ–‡ä»¶
        documents = self.load_documents(input_dir, required_exts)
        
        # å»ºç«‹ç´¢å¼•
        self.create_qdrant_index(documents, collection_name)
        
        # å»ºç«‹æŸ¥è©¢å¼•æ“Ž
        self.create_query_engine()
        
        # åŸ·è¡ŒæŸ¥è©¢
        return self.query(question)
